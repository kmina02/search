# -*- coding: utf-8 -*-
"""체험단 자동 검색 프로그램

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yph0BiSPawQCvxJr84RUXZ6euP494c9p
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import smtplib

"""# '리뷰노트' 사이트 자동 검색(미완성)

## 웹 스크래핑으로 상세검색 페이지 불러오기
검색 결과 페이지의 HTML 가져오기: requests.get 함수
"""

key = '연남'
url = "https://www.reviewnote.co.kr/campaigns?search={}"
r = requests.get(url.format(key))
soup = BeautifulSoup(r.text, 'html.parser')
soup

"""원하는 정보가 들어있는 태그를 리스트로 가져오기: soup.find_all() 메서드"""

#검색 결과에서 특정 문구를 포함한 요소 찾기
div_tags = soup.find_all('div', {'class': 'relative pl-[2.5px]'})
div_tags[0]

"""# '서울오빠' 사이트 자동 검색

## 웹 스크래핑으로 상세검색 페이지 불러오기
검색 결과 페이지의 HTML 가져오기: requests.get 함수
"""

key = '서울대입구'
url = "https://www.seoulouba.co.kr/campaign/?q={}"
r = requests.get(url.format(key))
soup = BeautifulSoup(r.text, 'html.parser')
print(soup)

"""원하는 정보가 들어있는 태그를 리스트로 가져오기: select() 메서드"""

#검색 결과에서 특정 문구를 포함한 요소 찾기
div_tags = soup.select('div.load_info')
div_tags[0]

"""## 체험단 리스트 출력
1. 마감된 체험단은 제외 - 완료
2. 체험단 이름, 제공내역, 링크 출력 - 완료
3. 위 정보를 담은 데이터프레임 만들기 - 완료
"""

# 반복되는 태그에 대해 함수 적용
def get_span(div_tags):
  global list
  list=[]
  global data
  for i in range(len(div_tags)):
    d_day = div_tags[i].find('div', attrs = {'class': "d_day"}).find('span') # d-day 정보가 들어있는 span 태그 찾기
    cpn = div_tags[i].find('a', attrs = {'target': '_blank'}) # a 태그 찾기
    cpn_name = cpn.find('strong').get_text() # cpn_name은 캠페인명
    cpn_inf = div_tags[i].find('span', attrs = {'class': "basic_blue"}).get_text() # cpn_inf은 캠페인 정보
    if '마감' not in d_day.get_text(): # 마감된 캠페인은 제외
      list.append([cpn_name, cpn_inf, cpn['href']]) # 캠페인 정보를 담은 리스트 생성
  return data = pd.DataFrame(list, columns = ['체험단명', '제공내역', '링크']) # 데이터프레임 생성

get_span(div_tags)

"""## '서울오빠' 사이트 검색함수 만들기
1. SeoulOuba(keyword) 함수 만들기 - 완료
2. 함수의 출력값: 캠페인이 나열된 데이터프레임 - 완료
3. 키워드가 2개 이상인 경우 함수 입력값을 리스트 형태로 받기 - 완료
"""

# 키워드가 하나인 경우
def SeoulOuba(keyword: str):
  url = "https://www.seoulouba.co.kr/campaign/?q={}"
  r = requests.get(url.format(keyword))
  soup = BeautifulSoup(r.text, 'html.parser')
  div_tags = soup.select('div.load_info')
  get_span(div_tags)
  print(data)

SeoulOuba('합정')

# 키워드가 두 개 이상인 경우(리스트 형태)
def SeoulOuba(keywords: list):
  global list
  list = []
  url = "https://www.seoulouba.co.kr/campaign/?q={}"
  for keyword in keywords:
    r = requests.get(url.format(keyword))
    soup = BeautifulSoup(r.text, 'html.parser')
    div_tags = soup.select('div.load_info')
    get_span(div_tags)
  print(data)

SeoulOuba(['용산', '연남', '홍대'])

"""## 자동 이메일 보내기
1. 이메일에 데이터프레임 담기
2. 이메일 기본 형식 지정
3. 매주 월요일 오전 9시에 이메일 보내기
"""

# 이메일 전송 정보 설정
smtp_server = 'smtp.naver.com'
smtp_port = 587
smtp_user = 'kmina8120@gmail.com'
smtp_password = 'alsdkrla8120'

# 이메일 작성
msg = MIMEMultipart()
msg['Subject'] = "/'서울대입구/' 검색결과" # 제목
msg['From'] = smtp_user
msg['To'] = 'kmina02@naver.com' # 받는 사람 이메일 주소
msg.attach(MIMEText(data.to_string(), 'plain')) # 본문

# SMTP 서버 연결 설정
smtp = smtplib.SMTP(smtp_server, smtp_port)
smtp.starttls()
smtp.login(smtp_user, smtp_password)

# 이메일 발송
smtp.sendmail(smtp_user, to_email, msg.as_string())
smtp.quit()

print('이메일이 전송되었습니다.')

"""# '디너의여왕' 사이트 자동 검색

## 웹 스크래핑으로 상세검색 페이지 불러오기
검색 결과 페이지의 HTML 가져오기: requests.get 함수
"""

key = '서울대입구'
url = "https://dinnerqueen.net/taste?query={}"
r = requests.get(url.format(key))
soup = BeautifulSoup(r.text, 'html.parser')
print(r.text)

"""원하는 정보가 들어있는 태그를 리스트로 가져오기: select() 메서드"""

#검색 결과에서 특정 문구를 포함한 요소 찾기
a_tags = soup.select('a.qz-dq-card__link')
a_tags[0]

# 상세검색 페이지에서 제공내역 안 나와서
# one-depth 더 들어가야 해
url_list = []
prv_list = []
for i in range(len(a_tags)):
  url="https://dinnerqueen.net" + a_tags[i]['href']
  r = requests.get(url)
  soup = BeautifulSoup(r.text, 'html.parser')
  tag1 = soup.find('div', attrs={'class': 'qz-col pc5 pc-r5 tb6 tb-r0'})
  tag2 = tag.find('p', attrs={'class': 'qz-body-kr mb-qz-body2-kr'})
  if tag2 is not None:
    prv = tag2.get_text()
  else:
    pass
  url_list.append(url)
  prv_list.append(prv)

"""## 체험단 리스트 출력
1. 마감된 체험단은 제외 - 완료
2. 체험단 이름, 제공내역, 링크 출력 - 완료
3. 위 정보를 담은 데이터프레임 만들기 - 완료
"""

url = "https://dinnerqueen.net" + a_tags[0]['href']
r = requests.get(url)
soup = BeautifulSoup(r.text, 'html.parser')
soup.find('div', attrs={'class': 'qz-col pc5 pc-r5 tb6 tb-r0'}).find('p').get_text()

# 반복되는 태그에 대해 함수 적용
def get_span(a_tags):
  global list
  global data
  for i in range(len(a_tags)):
    d_day = a_tags[i].find('div', attrs = {'class': "d_day"}).find('span') # d-day 정보가 들어있는 span 태그 찾기
    cpn = div_tags[i].find('a', attrs = {'target': '_blank'}) # a 태그 찾기
    cpn_name = cpn.find('strong').get_text() # cpn_name은 캠페인명
    cpn_inf = div_tags[i].find('span', attrs = {'class': "basic_blue"}).get_text() # cpn_inf은 캠페인 정보
    if '마감' not in d_day.get_text(): # 마감된 캠페인은 제외
      list.append([cpn_name, cpn_inf, cpn['href']]) # 캠페인 정보를 담은 리스트 생성
  data = pd.DataFrame(list, columns = ['체험단명', '제공내역', '링크']) # 데이터프레임 생성

a_tags[0]

